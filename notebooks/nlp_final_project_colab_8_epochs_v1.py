# -*- coding: utf-8 -*-
"""NLP_Final_Project_Colab_8_epochs_v1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bnqndAMarxxqitcZmTYCMAEjUrH0Zt3v

# NLP Final Project: Dataset Cartography for Artifact Mitigation
## Fast GPU Training in Google Colab

This notebook runs the complete training pipeline using GPU acceleration for fast results.

## 1. Setup Environment
"""

# Check GPU availability
import torch
print(f"CUDA available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"GPU: {torch.cuda.get_device_name(0)}")
    print(f"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")

# Install required packages
!pip install datasets transformers torch evaluate matplotlib seaborn scipy

# Commented out IPython magic to ensure Python compatibility.
# Clone repository
!git clone https://github.com/agsilver108/nlp-fa25-final-project.git
# %cd nlp-fa25-final-project

"""## 2. Training Setup and Execution

The notebook will now:
1. Pull the latest fixes from GitHub
2. Execute the optimized `colab_training.py` script
3. Train both baseline and cartography-mitigated models
4. Generate comparison results with proper evaluation metrics

**Expected Results:**
- Baseline F1: 60-80% (good performance)
- Training time: 25-35 minutes with GPU acceleration
- Cartography comparison to show artifact mitigation effectiveness
"""

# Pull latest changes and run the COMPLETELY FIXED training pipeline
print("üîß Pulling latest fixes from repository...")
!git pull origin main

print("üöÄ Starting training with COMPLETELY FIXED script...")
print("‚è±Ô∏è  Expected training time: 25-35 minutes (8 epochs with evaluation)")
print("üìä Using colab_training_fixed.py with extensive error handling!")
print("üîß This version should definitely show proper EM and F1 scores!")

# Import the helpers module
import sys
sys.path.append('./scripts')
import helpers

# Execute the completely fixed training script
# Assuming run_training is defined directly in colab_training_final.py
# The script will now call run_training directly instead of helpers.run_training
exec(open('colab_assist/colab_training_final.py').read())

"""## 3. View Results"""

# Load and display results with better error handling
import json
import os

results_file = '/content/colab_training_results.json'

if os.path.exists(results_file):
    with open(results_file, 'r') as f:
        results = json.load(f)

    print("üéØ Training Results Summary:")
    print(f"\nBaseline Model:")
    print(f"  Exact Match: {results['baseline']['exact_match']:.3f}")
    print(f"  F1 Score: {results['baseline']['f1']:.3f}")
    print(f"  Training Time: {results['baseline']['training_time']:.1f}s")

    print(f"\nCartography Model:")
    print(f"  Exact Match: {results['cartography']['exact_match']:.3f}")
    print(f"  F1 Score: {results['cartography']['f1']:.3f}")
    print(f"  Training Time: {results['cartography']['training_time']:.1f}s")

    print(f"\nImprovement:")
    print(f"  EM Diff: {results['improvement']['em_diff']:+.3f}")
    print(f"  F1 Diff: {results['improvement']['f1_diff']:+.3f}")

    # Quality assessment
    baseline_f1 = results['baseline']['f1']
    if baseline_f1 > 0.7:
        print(f"\n‚úÖ EXCELLENT: Baseline F1 > 70% indicates good training!")
    elif baseline_f1 > 0.5:
        print(f"\n‚úîÔ∏è  GOOD: Baseline F1 > 50% indicates decent training.")
    elif baseline_f1 > 0.2:
        print(f"\n‚ö†Ô∏è  OKAY: Baseline F1 > 20% indicates some learning occurred.")
    elif baseline_f1 > 0:
        print(f"\n‚ùå POOR: Baseline F1 very low, model barely learning.")
    else:
        print(f"\nüíÄ BROKEN: F1 = 0 indicates evaluation is not working!")

else:
    print("‚ùå Results file not found!")
    print("This could mean:")
    print("1. Training hasn't completed yet")
    print("2. Training failed with an error")
    print("3. The training script couldn't save results")
    print("\nCheck the training output above for error messages.")

"""## 3.1. Visualize Training Results

Create beautiful charts to compare baseline vs cartography-mitigated model performance.
"""

# Create beautiful visualizations of training results
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import json
import os

# Set up the plotting style
plt.style.use('default')
sns.set_palette("husl")

results_file = '/content/colab_training_results.json'

if os.path.exists(results_file):
    with open(results_file, 'r') as f:
        results = json.load(f)

    # Create a comprehensive visualization dashboard
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))
    fig.suptitle('üéØ NLP Final Project: Dataset Cartography Results Dashboard', fontsize=16, fontweight='bold')

    # 1. Performance Comparison Bar Chart
    models = ['Baseline', 'Cartography']
    em_scores = [
        results['baseline']['exact_match'],
        results.get('cartography', {}).get('exact_match', 0) if results.get('cartography') else 0
    ]
    f1_scores = [
        results['baseline']['f1'],
        results.get('cartography', {}).get('f1', 0) if results.get('cartography') else 0
    ]

    x = np.arange(len(models))
    width = 0.35

    bars1 = ax1.bar(x - width/2, em_scores, width, label='Exact Match', color='#FF6B6B', alpha=0.8)
    bars2 = ax1.bar(x + width/2, f1_scores, width, label='F1 Score', color='#4ECDC4', alpha=0.8)

    ax1.set_xlabel('Models', fontweight='bold')
    ax1.set_ylabel('Score', fontweight='bold')
    ax1.set_title('üìä Performance Comparison: EM vs F1', fontweight='bold', pad=20)
    ax1.set_xticks(x)
    ax1.set_xticklabels(models)
    ax1.legend()
    ax1.set_ylim(0, 1)
    ax1.grid(True, alpha=0.3)

    # Add value labels on bars
    for bar in bars1:
        height = bar.get_height()
        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                f'{height:.3f}', ha='center', va='bottom', fontweight='bold')
    for bar in bars2:
        height = bar.get_height()
        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                f'{height:.3f}', ha='center', va='bottom', fontweight='bold')

    # 2. Training Time Comparison (if cartography data available)
    if results.get('cartography'):
        times = [
            results['baseline']['training_time'],
            results['cartography']['training_time']
        ]
        colors = ['#FFD93D', '#6BCF7F']

        wedges, texts, autotexts = ax2.pie(times, labels=models, autopct='%1.1f%%',
                                          colors=colors, startangle=90, explode=(0.05, 0.05))
        ax2.set_title('‚è±Ô∏è Training Time Distribution', fontweight='bold', pad=20)

        # Make percentage text bold
        for autotext in autotexts:
            autotext.set_color('white')
            autotext.set_fontweight('bold')
            autotext.set_fontsize(12)
    else:
        ax2.text(0.5, 0.5, '‚è≥ Cartography\nTraining\nNot Completed',
                ha='center', va='center', transform=ax2.transAxes,
                fontsize=14, fontweight='bold',
                bbox=dict(boxstyle="round,pad=0.3", facecolor='lightgray', alpha=0.7))
        ax2.set_title('‚è±Ô∏è Training Time Distribution', fontweight='bold', pad=20)

    # 3. Improvement Metrics (if cartography data available)
    if results.get('cartography') and results.get('improvement'):
        improvements = [
            results['improvement']['em_diff'],
            results['improvement']['f1_diff']
        ]
        metrics = ['EM Improvement', 'F1 Improvement']
        colors = ['#FF9FF3' if x >= 0 else '#FF6B6B' for x in improvements]

        bars3 = ax3.bar(metrics, improvements, color=colors, alpha=0.8)
        ax3.set_ylabel('Improvement Score', fontweight='bold')
        ax3.set_title('üìà Cartography vs Baseline Improvement', fontweight='bold', pad=20)
        ax3.axhline(y=0, color='black', linestyle='-', alpha=0.3)
        ax3.grid(True, alpha=0.3)

        # Add value labels
        for bar, val in zip(bars3, improvements):
            height = bar.get_height()
            ax3.text(bar.get_x() + bar.get_width()/2.,
                    height + (0.005 if height >= 0 else -0.015),
                    f'{val:+.3f}', ha='center',
                    va='bottom' if height >= 0 else 'top',
                    fontweight='bold')
    else:
        ax3.text(0.5, 0.5, 'üìä Improvement\nAnalysis\nPending',
                ha='center', va='center', transform=ax3.transAxes,
                fontsize=14, fontweight='bold',
                bbox=dict(boxstyle="round,pad=0.3", facecolor='lightblue', alpha=0.7))
        ax3.set_title('üìà Cartography vs Baseline Improvement', fontweight='bold', pad=20)

    # 4. Performance Quality Assessment
    baseline_f1 = results['baseline']['f1']
    cartography_f1 = results.get('cartography', {}).get('f1', 0) if results.get('cartography') else 0

    # Create quality categories
    quality_levels = ['Poor\n(0-20%)', 'Okay\n(20-50%)', 'Good\n(50-70%)', 'Excellent\n(70%+)']
    quality_ranges = [0.2, 0.5, 0.7, 1.0]

    # Determine quality for each model
    def get_quality_level(f1_score):
        if f1_score <= 0.2:
            return 0
        elif f1_score <= 0.5:
            return 1
        elif f1_score <= 0.7:
            return 2
        else:
            return 3

    baseline_quality = get_quality_level(baseline_f1)
    cartography_quality = get_quality_level(cartography_f1) if cartography_f1 > 0 else 0

    # Create stacked bar chart for quality assessment
    quality_colors = ['#FF4444', '#FFA500', '#90EE90', '#32CD32']

    # Show current performance levels
    ax4.barh(['Baseline'], [1], color=quality_colors[baseline_quality], alpha=0.8, height=0.4)
    if cartography_f1 > 0:
        ax4.barh(['Cartography'], [1], color=quality_colors[cartography_quality], alpha=0.8, height=0.4)

    ax4.set_xlim(0, 1)
    ax4.set_xlabel('Performance Quality', fontweight='bold')
    ax4.set_title('üéØ Model Quality Assessment', fontweight='bold', pad=20)

    # Add performance level labels
    ax4.text(0.5, 0, f'F1: {baseline_f1:.3f}\n{quality_levels[baseline_quality]}',
            ha='center', va='center', fontweight='bold', color='white')
    if cartography_f1 > 0:
        ax4.text(0.5, 1, f'F1: {cartography_f1:.3f}\n{quality_levels[cartography_quality]}',
                ha='center', va='center', fontweight='bold', color='white')

    # Add legend for quality levels
    legend_elements = [plt.Rectangle((0,0),1,1, facecolor=color, alpha=0.8)
                      for color in quality_colors]
    ax4.legend(legend_elements, quality_levels, loc='center left', bbox_to_anchor=(1, 0.5))

    plt.tight_layout()
    plt.show()

    # Additional detailed summary
    print("\n" + "="*60)
    print("üìä DETAILED RESULTS SUMMARY")
    print("="*60)

    print(f"\nüéØ BASELINE MODEL:")
    print(f"   ‚Ä¢ Exact Match: {results['baseline']['exact_match']:.1%}")
    print(f"   ‚Ä¢ F1 Score: {results['baseline']['f1']:.1%}")
    print(f"   ‚Ä¢ Training Time: {results['baseline']['training_time']:.1f} seconds")
    print(f"   ‚Ä¢ Quality Level: {quality_levels[baseline_quality]}")

    if results.get('cartography'):
        print(f"\nüó∫Ô∏è CARTOGRAPHY MODEL:")
        print(f"   ‚Ä¢ Exact Match: {results['cartography']['exact_match']:.1%}")
        print(f"   ‚Ä¢ F1 Score: {results['cartography']['f1']:.1%}")
        print(f"   ‚Ä¢ Training Time: {results['cartography']['training_time']:.1f} seconds")
        print(f"   ‚Ä¢ Quality Level: {quality_levels[cartography_quality]}")

        if results.get('improvement'):
            print(f"\nüìà IMPROVEMENT ANALYSIS:")
            em_change = results['improvement']['em_diff']
            f1_change = results['improvement']['f1_diff']
            print(f"   ‚Ä¢ EM Change: {em_change:+.1%} ({'‚úÖ Better' if em_change > 0 else '‚ùå Worse' if em_change < 0 else '‚ûñ Same'})")
            print(f"   ‚Ä¢ F1 Change: {f1_change:+.1%} ({'‚úÖ Better' if f1_change > 0 else '‚ùå Worse' if f1_change < 0 else '‚ûñ Same'})")

            if em_change > 0 or f1_change > 0:
                print(f"   üéâ Dataset cartography shows positive impact on artifact mitigation!")
            elif em_change == 0 and f1_change == 0:
                print(f"   ‚ûñ Dataset cartography shows no significant change.")
            else:
                print(f"   ‚ö†Ô∏è Dataset cartography may need tuning - negative impact observed.")

    print("\n" + "="*60)

else:
    print("‚ùå No results file found!")
    print("Please run the training first to generate results for visualization.")

    # Create a placeholder visualization
    fig, ax = plt.subplots(1, 1, figsize=(10, 6))
    ax.text(0.5, 0.5, 'üìä Training Results\nVisualization\n\nRun training first to see\nbeautiful charts here!',
            ha='center', va='center', transform=ax.transAxes,
            fontsize=20, fontweight='bold',
            bbox=dict(boxstyle="round,pad=0.5", facecolor='lightcoral', alpha=0.7))
    ax.set_title('üéØ NLP Final Project Results Dashboard', fontsize=16, fontweight='bold', pad=20)
    ax.axis('off')
    plt.tight_layout()
    plt.show()

"""## 4. Download Results"""

# Download trained models and results
from google.colab import files

# Zip results for download
!zip -r colab_results.zip /content/baseline_model /content/cartography_model /content/colab_training_results.json
files.download('colab_results.zip')